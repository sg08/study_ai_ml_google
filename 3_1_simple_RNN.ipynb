{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b462010-d9eb-4159-be58-7a6b1b1c780c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code_colab_lesson_3_4')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXB1SiLP4OL"
      },
      "source": [
        "# simple RNN\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tzSWNYwxP4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dae0b6fc-0ed6-4aef-92ce-f8c2c821aaaf"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:0.8552922918710246\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "11 + 52 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.7445483269958576\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "70 + 120 = 239\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.8860925218578657\n",
            "Pred:[1 1 0 1 1 0 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "105 + 101 = 218\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.1138523882941238\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "113 + 116 = 130\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.041361608268851\n",
            "Pred:[0 0 0 0 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "43 + 69 = 2\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.8372108798906186\n",
            "Pred:[0 0 1 1 0 1 0 0]\n",
            "True:[0 0 0 1 1 1 0 0]\n",
            "2 + 26 = 52\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.2699089634921377\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "69 + 123 = 255\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.939796825000861\n",
            "Pred:[1 1 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "61 + 121 = 246\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.1062775712639152\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "93 + 42 = 254\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.8444812892714983\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "56 + 72 = 80\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0808040356535553\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "126 + 82 = 252\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.8976819507011774\n",
            "Pred:[0 0 0 0 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "11 + 33 = 15\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.0210369186652717\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "27 + 40 = 50\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.8659950998654212\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "42 + 65 = 77\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.7679400863066556\n",
            "Pred:[0 0 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "70 + 3 = 13\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.8033142649971526\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "9 + 81 = 94\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.8634606436510404\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "103 + 36 = 75\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.7160255165801918\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 1 0 1 1 0 1 0]\n",
            "116 + 102 = 138\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.138938968201587\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "59 + 45 = 86\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.8374657628424084\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "23 + 67 = 92\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8436029865416746\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "5 + 28 = 57\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.8379669754158308\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "60 + 68 = 72\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.2061278359854728\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "59 + 125 = 254\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.5954945989011255\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "52 + 68 = 88\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.6582397234972782\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "57 + 8 = 81\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.40717336326541254\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "65 + 39 = 104\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.6066745616503431\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "9 + 123 = 148\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.25856616778252695\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "55 + 36 = 91\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.4084275286295067\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "120 + 17 = 137\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.22816263964662734\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "40 + 68 = 108\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.36927540904652273\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "92 + 12 = 104\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.2063544985363946\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "105 + 43 = 148\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.1661121358252357\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "45 + 36 = 81\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.29884750173399244\n",
            "Pred:[1 1 1 0 0 0 0 1]\n",
            "True:[1 1 1 0 0 0 0 1]\n",
            "122 + 103 = 225\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.24557240829789403\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "100 + 99 = 199\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.9656553889355457\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "76 + 3 = 83\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.08959525373875797\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "126 + 20 = 146\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.06743243337464444\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "49 + 70 = 119\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.25281252068722465\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "78 + 9 = 83\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.2232105562767478\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "39 + 21 = 60\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.08367242469081355\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "36 + 110 = 146\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.05764894727212863\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "12 + 85 = 97\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.0902841920349329\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "124 + 76 = 200\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.05338394564771442\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "65 + 46 = 111\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.037700883356868574\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "36 + 104 = 140\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.06627525429613185\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "116 + 30 = 146\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.0364958039462958\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "96 + 111 = 207\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.04212849438480347\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "30 + 94 = 124\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.026959332609408915\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "110 + 102 = 212\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.012610935773189993\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "8 + 97 = 105\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.009163066028158338\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "66 + 0 = 66\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.009818073948449241\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "106 + 4 = 110\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.008856052143967933\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "19 + 85 = 104\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.021742254813629378\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "118 + 41 = 159\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.014363258208455507\n",
            "Pred:[1 1 0 0 1 0 1 1]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "98 + 105 = 203\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.017270862976758783\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "25 + 123 = 148\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.0127593931147983\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "112 + 27 = 139\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.007854406486448582\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "92 + 1 = 93\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.007116415757280465\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "92 + 96 = 188\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.009124609581697616\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "93 + 97 = 190\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.007667338812599126\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "92 + 18 = 110\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.02114388850346555\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "55 + 91 = 146\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.006273840890706724\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "104 + 70 = 174\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.019170653408631902\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "89 + 55 = 144\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.008687509817684232\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "43 + 77 = 120\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.009843765021150194\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "23 + 51 = 74\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.004690448895165736\n",
            "Pred:[0 0 1 0 1 0 1 1]\n",
            "True:[0 0 1 0 1 0 1 1]\n",
            "36 + 7 = 43\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.006582758274185439\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "36 + 78 = 114\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.006145843636598257\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "114 + 42 = 156\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.002947087156284858\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "113 + 70 = 183\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.007323230775996526\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "94 + 4 = 98\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.004060544722550642\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "80 + 27 = 107\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.004892236062357876\n",
            "Pred:[1 1 0 0 1 1 0 1]\n",
            "True:[1 1 0 0 1 1 0 1]\n",
            "93 + 112 = 205\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.008319967797266683\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "127 + 87 = 214\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0034396724347869156\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "75 + 100 = 175\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.007835781987753733\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "42 + 119 = 161\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.006037195926690192\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "127 + 19 = 146\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.002908626095859805\n",
            "Pred:[0 0 1 1 0 0 0 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "25 + 24 = 49\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.006656990648756574\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "73 + 59 = 132\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0038034221030027577\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "69 + 79 = 148\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0028965798123496184\n",
            "Pred:[0 0 0 1 1 1 1 1]\n",
            "True:[0 0 0 1 1 1 1 1]\n",
            "16 + 15 = 31\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.002521992483033767\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "25 + 74 = 99\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.003964373146494996\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "95 + 46 = 141\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.00242437026504995\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "53 + 120 = 173\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0008414769351317213\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "45 + 69 = 114\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.005956343787792262\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "31 + 111 = 142\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0016256729062994236\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "80 + 37 = 117\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0022323971738028234\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "49 + 60 = 109\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.004170969776880939\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "95 + 115 = 210\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.00239354955431045\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "92 + 80 = 172\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0028134061885831483\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "119 + 88 = 207\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.002316483250885235\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "53 + 58 = 111\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.002690421105606701\n",
            "Pred:[1 1 1 0 1 0 1 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "114 + 120 = 234\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0021546334098006942\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "43 + 100 = 143\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0010066439809995455\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "91 + 89 = 180\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.001169531616118808\n",
            "Pred:[0 0 0 1 1 1 0 1]\n",
            "True:[0 0 0 1 1 1 0 1]\n",
            "13 + 16 = 29\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0017089010886748653\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "29 + 24 = 53\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0013259479284217668\n",
            "Pred:[0 1 0 0 1 0 0 1]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "54 + 19 = 73\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0015859655756122635\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "18 + 49 = 67\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0011254533541139092\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "41 + 121 = 162\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zQEPrtP4OP"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}